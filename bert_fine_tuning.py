# -*- coding: utf-8 -*-
"""BERT_fine_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rwkKYyylkFpz4dtzQwq-4r95mn-um2Lp
"""

"""## BERT"""

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

def get_pad_mask(tokens, i_pad=0):
    """
    pad mask ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    :param tokens: tokens (bs, n_seq)
    :param i_pad: id of pad
    :return mask: pad mask (pad: 1, other: 0)
    """
    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)
    mask = tf.expand_dims(mask, axis=1)
    return mask


def get_ahead_mask(tokens, i_pad=0):
    """
    ahead mask ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    :param tokens: tokens (bs, n_seq)
    :param i_pad: id of pad
    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)
    """
    n_seq = tf.shape(tokens)[1]
    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)
    ahead_mask = tf.expand_dims(ahead_mask, axis=0)
    pad_mask = get_pad_mask(tokens, i_pad)
    mask = tf.maximum(ahead_mask, pad_mask)
    return mask


@tf.function(experimental_relax_shapes=True)
def gelu(x):
    """
    gelu activation í•¨ìˆ˜
    :param x: ì…ë ¥ ê°’
    :return: gelu activation result
    """
    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))


def kernel_initializer(stddev=0.02):
    """
    parameter initializer ìƒì„±
    :param stddev: ìƒì„±í•  ëœë¤ ë³€ìˆ˜ì˜ í‘œì¤€í¸ì°¨
    """
    return tf.keras.initializers.TruncatedNormal(stddev=stddev)


def bias_initializer():
    """
    bias initializer ìƒì„±
    """
    return tf.zeros_initializer


class Config(dict):
    """
    jsonì„ config í˜•íƒœë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ Class
    :param dict: config dictionary
    """
    __getattr__ = dict.__getitem__
    __setattr__ = dict.__setitem__

    @classmethod
    def load(cls, file):
        """
        fileì—ì„œ Configë¥¼ ìƒì„± í•¨
        :param file: filename
        """
        with open(file, 'r') as f:
            config = json.loads(f.read())
            return Config(config)

# mode == "embedding" ì¼ ê²½ìš° Token Embedding Layer ë¡œ ì‚¬ìš©ë˜ëŠ” layer í´ë˜ìŠ¤ì…ë‹ˆë‹¤. 

class SharedEmbedding(tf.keras.layers.Layer):
    """
    Weighed Shared Embedding Class
    """
    def __init__(self, config, name="weight_shared_embedding"):
        """
        ìƒì„±ì
        :param config: Config ê°ì²´
        :param name: layer name
        """
        super().__init__(name=name)

        self.n_vocab = config.n_vocab
        self.d_model = config.d_model
    
    def build(self, input_shape):
        """
        shared weight ìƒì„±
        :param input_shape: Tensor Shape (not used)
        """
        with tf.name_scope("shared_embedding_weight"):
            self.shared_weights = self.add_weight(
                "weights",
                shape=[self.n_vocab, self.d_model],
                initializer=kernel_initializer()
            )

    def call(self, inputs, mode="embedding"):
        """
        layer ì‹¤í–‰
        :param inputs: ì…ë ¥
        :param mode: ì‹¤í–‰ ëª¨ë“œ
        :return: embedding or linear ì‹¤í–‰ ê²°ê³¼
        """
        # modeê°€ embeddingì¼ ê²½ìš° embedding lookup ì‹¤í–‰
        if mode == "embedding":
            return self._embedding(inputs)
        # modeê°€ linearì¼ ê²½ìš° linear ì‹¤í–‰
        elif mode == "linear":
            return self._linear(inputs)
        # modeê°€ ê¸°íƒ€ì¼ ê²½ìš° ì˜¤ë¥˜ ë°œìƒ
        else:
            raise ValueError(f"mode {mode} is not valid.")
    
    def _embedding(self, inputs):
        """
        embedding lookup
        :param inputs: ì…ë ¥
        """
        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))
        return embed

    def _linear(self, inputs):  # (bs, n_seq, d_model)
        """
        linear ì‹¤í–‰
        :param inputs: ì…ë ¥
        """
        n_batch = tf.shape(inputs)[0]
        n_seq = tf.shape(inputs)[1]
        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)
        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)
        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)
        return outputs

class PositionalEmbedding(tf.keras.layers.Layer):
    """
    Positional Embedding Class
    """
    def __init__(self, config, name="position_embedding"):
        """
        ìƒì„±ì
        :param config: Config ê°ì²´
        :param name: layer name
        """
        super().__init__(name=name)
        
        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())

    def call(self, inputs):
        """
        layer ì‹¤í–‰
        :param inputs: ì…ë ¥
        :return embed: positional embedding lookup ê²°ê³¼
        """
        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)
        embed = self.embedding(position)
        return embed

class ScaleDotProductAttention(tf.keras.layers.Layer):
    """
    Scale Dot Product Attention Class
    """
    def __init__(self, name="scale_dot_product_attention"):
        """
        ìƒì„±ì
        :param name: layer name
        """
        super().__init__(name=name)

    def call(self, Q, K, V, attn_mask):
        """
        layer ì‹¤í–‰
        :param Q: Q value
        :param K: K value
        :param V: V value
        :param attn_mask: ì‹¤í–‰ ëª¨ë“œ
        :return attn_out: attention ì‹¤í–‰ ê²°ê³¼
        """
        attn_score = tf.matmul(Q, K, transpose_b=True)
        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))
        attn_scale = tf.math.divide(attn_score, scale)
        attn_scale -= 1.e9 * attn_mask
        attn_prob = tf.nn.softmax(attn_scale, axis=-1)
        attn_out = tf.matmul(attn_prob, V)
        return attn_out

class MultiHeadAttention(tf.keras.layers.Layer):
    """
    Multi Head Attention Class
    """
    def __init__(self, config, name="multi_head_attention"):
        """
        ìƒì„±ì
        :param config: Config ê°ì²´
        :param name: layer name
        """
        super().__init__(name=name)

        self.d_model = config.d_model
        self.n_head = config.n_head
        self.d_head = config.d_head

        # Q, K, V input dense layer
        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())
        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())
        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())
        # Scale Dot Product Attention class
        self.attention = ScaleDotProductAttention(name="self_attention")
        # output dense layer
        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())

    def call(self, Q, K, V, attn_mask):
        """
        layer ì‹¤í–‰
        :param Q: Q value
        :param K: K value
        :param V: V value
        :param attn_mask: ì‹¤í–‰ ëª¨ë“œ
        :return attn_out: attention ì‹¤í–‰ ê²°ê³¼
        """
        # reshape Q, K, V, attn_mask
        batch_size = tf.shape(Q)[0]
        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)
        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)
        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)
        attn_mask_m = tf.expand_dims(attn_mask, axis=1)
        # Scale Dot Product Attention with multi head Q, K, V, attn_mask
        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)
        # transpose and liner
        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)
        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)
        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)

        return attn_out

class PositionWiseFeedForward(tf.keras.layers.Layer):
    """
    Position Wise Feed Forward Class
    """
    def __init__(self, config, name="feed_forward"):
        """
        ìƒì„±ì
        :param config: Config ê°ì²´
        :param name: layer name
        """
        super().__init__(name=name)

        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())
        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())

    def call(self, inputs):
        """
        layer ì‹¤í–‰
        :param inputs: inputs
        :return ff_val: feed forward ì‹¤í–‰ ê²°ê³¼
        """
        ff_val = self.W_2(self.W_1(inputs))
        return ff_val

class EncoderLayer(tf.keras.layers.Layer):
    """
    Encoder Layer Class
    """
    def __init__(self, config, name="encoder_layer"):
        """
        ìƒì„±ì
        :param config: Config ê°ì²´
        :param name: layer name
        """
        super().__init__(name=name)

        self.self_attention = MultiHeadAttention(config)
        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)

        self.ffn = PositionWiseFeedForward(config)
        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)

        self.dropout = tf.keras.layers.Dropout(config.dropout)
 
    def call(self, enc_embed, self_mask):
        """
        layer ì‹¤í–‰
        :param enc_embed: enc_embed ë˜ëŠ” ì´ì „ EncoderLayerì˜ ì¶œë ¥
        :param self_mask: enc_tokensì˜ pad mask
        :return enc_out: EncoderLayer ì‹¤í–‰ ê²°ê³¼
        """
        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)
        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))

        ffn_val = self.ffn(norm1_val)
        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))

        return enc_out

class BERT(tf.keras.layers.Layer):
    """
    BERT Class
    """
    def __init__(self, config, name="bert"):
        """
        ìƒì„±ì
        :param config: Config ê°ì²´
        :param name: layer name
        """
        super().__init__(name=name)

        self.i_pad = config.i_pad
        self.embedding = SharedEmbedding(config)
        self.position = PositionalEmbedding(config)
        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())
        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)
        
        self.encoder_layers = [EncoderLayer(config, name=f"encoder_layer_{i}") for i in range(config.n_layer)]

        self.dropout = tf.keras.layers.Dropout(config.dropout)

    def call(self, enc_tokens, segments):
        """
        layer ì‹¤í–‰
        :param enc_tokens: encoder tokens
        :param segments: token segments
        :return logits_cls: CLS ê²°ê³¼ logits
        :return logits_lm: LM ê²°ê³¼ logits
        """
        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)

        enc_embed = self.get_embedding(enc_tokens, segments)

        enc_out = self.dropout(enc_embed)
        for encoder_layer in self.encoder_layers:
            enc_out = encoder_layer(enc_out, enc_self_mask)

        logits_cls = enc_out[:,0]
        logits_lm = enc_out
        return logits_cls, logits_lm
    
    def get_embedding(self, tokens, segments):
        """
        token embedding, position embedding lookup
        :param tokens: ì…ë ¥ tokens
        :param segments: ì…ë ¥ segments
        :return embed: embedding ê²°ê³¼
        """
        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)
        embed = self.norm(embed)
        return embed

class BERT(tf.keras.layers.Layer):
    """
    BERT Class
    """
    def __init__(self, config, name="bert"):
        """
        ìƒì„±ì
        :param config: Config ê°ì²´
        :param name: layer name
        """
        super().__init__(name=name)

        self.i_pad = config.i_pad
        self.embedding = SharedEmbedding(config)
        self.position = PositionalEmbedding(config)
        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())
        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)
        
        self.encoder_layers = [EncoderLayer(config, name=f"encoder_layer_{i}") for i in range(config.n_layer)]

        self.dropout = tf.keras.layers.Dropout(config.dropout)

    def call(self, enc_tokens, segments):
        """
        layer ì‹¤í–‰
        :param enc_tokens: encoder tokens
        :param segments: token segments
        :return logits_cls: CLS ê²°ê³¼ logits
        :return logits_lm: LM ê²°ê³¼ logits
        """
        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)

        enc_embed = self.get_embedding(enc_tokens, segments)

        enc_out = self.dropout(enc_embed)
        for encoder_layer in self.encoder_layers:
            enc_out = encoder_layer(enc_out, enc_self_mask)

        logits_cls = enc_out[:,0]
        logits_lm = enc_out
        return logits_cls, logits_lm
    
    def get_embedding(self, tokens, segments):
        """
        token embedding, position embedding lookup
        :param tokens: ì…ë ¥ tokens
        :param segments: ì…ë ¥ segments
        :return embed: embedding ê²°ê³¼
        """
        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)
        embed = self.norm(embed)
        return embed

"""### BERT ëª¨ë¸ì„ ì´ìš©í•œ ë„ì „Â¶

BERT ëª¨ë¸ì„ í™œìš©í•˜ì—¬, LSTMìœ¼ë¡œ í’€ì–´ë³´ì•˜ë˜ KorQuAD íƒœìŠ¤í¬ë¥¼ ë‹¤ì‹œ í•™ìŠµí•´ ë³´ì.
ëª¨ë¸ì˜ ì°¨ì´ë§Œ ë¹„êµí•´ ë³´ê¸° ìœ„í•´ ì¼ë¶€ëŸ¬ ë‘ ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” Tokenizerë¥¼ ë™ì¼í•˜ê²Œ êµ¬ì„±í•˜ë‹¤.
ì•„ë˜ëŠ” BERT ë ˆì´ì–´ì— Fully Connected layerë¥¼ ë¶™ì–´ KorQuADìš©ìœ¼ë¡œ finetuneí•˜ê¸° ìœ„í•œ ëª¨ë¸ í´ë˜ìŠ¤ì´ë‹¤.
"""

class BERT4KorQuAD(tf.keras.Model):
    def __init__(self, config):
        super().__init__(name='BERT4KorQuAD')

        self.bert = BERT(config)
        self.dense = tf.keras.layers.Dense(2)
    
    def call(self, enc_tokens, segments):
        logits_cls, logits_lm = self.bert(enc_tokens, segments)

        hidden = self.dense(logits_lm) # (bs, n_seq, 2)
        start_logits, end_logits = tf.split(hidden, 2, axis=-1)  # (bs, n_seq, 1), (bs, n_seq, 1)

        start_logits = tf.squeeze(start_logits, axis=-1)
        start_outputs = tf.keras.layers.Softmax(name="start")(start_logits)

        end_logits = tf.squeeze(end_logits, axis=-1)
        end_outputs = tf.keras.layers.Softmax(name="end")(end_logits)

        return start_outputs, end_outputs

config = Config({"d_model": 256, "n_head": 4, "d_head": 64, "dropout": 0.1, "d_ff": 1024, "layernorm_epsilon": 0.001, "n_layer": 3, "n_seq": 384, "n_vocab": 0, "i_pad": 0})
config.n_vocab = len(vocab)
config.i_pad = vocab.pad_id()
config

from tensorflow.python.client import device_lib

print(device_lib.list_local_devices())

bert_batch_size = 32

with tf.device('/device:GPU:0'):
  train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels)).shuffle(10000).batch(bert_batch_size)
  dev_dataset = tf.data.Dataset.from_tensor_slices((dev_inputs, dev_labels)).batch(bert_batch_size)

model = BERT4KorQuAD(config)

def train_epoch(model, dataset, loss_fn, acc_fn, optimizer):
    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')
    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')
    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')
    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')

    p_bar = tqdm(dataset)
    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(p_bar):
        with tf.GradientTape() as tape:
            start_outputs, end_outputs = model(enc_tokens, segments)

            start_loss = loss_fn(start_labels, start_outputs)
            end_loss = loss_fn(end_labels, end_outputs)
            loss = start_loss + end_loss

            start_acc = acc_fn(start_labels, start_outputs)
            end_acc = acc_fn(end_labels, end_outputs)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))

        metric_start_loss(start_loss)
        metric_end_loss(end_loss)
        metric_start_acc(start_acc)
        metric_end_acc(end_acc)
        if batch % 10 == 9:
            p_bar.set_description(f'loss: {metric_start_loss.result():0.4f}, {metric_end_loss.result():0.4f}, acc: {metric_start_acc.result():0.4f}, {metric_end_acc.result():0.4f}')
    p_bar.close()

    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()

def eval_epoch(model, dataset, loss_fn, acc_fn):
    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')
    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')
    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')
    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')

    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(dataset):
        start_outputs, end_outputs = model(enc_tokens, segments)

        start_loss = loss_fn(start_labels, start_outputs)
        end_loss = loss_fn(end_labels, end_outputs)

        start_acc = acc_fn(start_labels, start_outputs)
        end_acc = acc_fn(end_labels, end_outputs)

        metric_start_loss(start_loss)
        metric_end_loss(end_loss)
        metric_start_acc(start_acc)
        metric_end_acc(end_acc)

    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()

loss_fn = tf.keras.losses.sparse_categorical_crossentropy
acc_fn = tf.keras.metrics.sparse_categorical_accuracy

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)

best_acc = .0
patience = 0
for epoch in range(1): ########
    train_epoch(model, train_dataset, loss_fn, acc_fn, optimizer)
    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, dev_dataset, loss_fn, acc_fn)
    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')
    acc = start_acc + end_acc
    if best_acc < acc:
        patience = 0
        best_acc = acc
        model.save_weights(os.path.join(data_dir, "korquad_bert_none_pretrain.hdf5"))
        print(f'save best model')
    else:
        patience += 1
    if 2 <= patience:
        print(f'early stopping')
        break

"""## 5. Pretrained modelì˜ í™œìš©Â¶

pretrained modelì„ í™œìš©í•´ ë³´ì.
ì‚¬ìš©í•´ì•¼ í•  ëª¨ë¸ êµ¬ì¡°ë‚˜ ë°ì´í„°ì…‹ êµ¬ì¡°, ë°°ì¹˜ êµ¬ì¡°ëŠ” ì´ì „ ìŠ¤í…ê³¼ ë™ì¼í•˜ë‹¤.
ì´ë¯¸ ë‹¤ìš´ë¡œë“œí•œ pretrained modelì„ í™œìš©í•˜ëŠ” í•™ìŠµì„ ë‹¤ì‹œ ì§„í–‰í•´ ë³´ì.

## STEP 1. pretrained model ë¡œë”©í•˜ê¸°
"""

checkpoint_file = os.path.join(model_dir, 'bert_pretrain_32000.hdf5')

with tf.device('device:GPU:0'):
  model = BERT4KorQuAD(config)

if os.path.exists(checkpoint_file):
    #  pretrained model ì„ ë¡œë“œí•˜ê¸° ìœ„í•´ ë¨¼ì € ëª¨ë¸ì´ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.
    enc_tokens = np.random.randint(0, len(vocab), (4, 10))
    segments = np.random.randint(0, 2, (4, 10))
    model(enc_tokens, segments)
    
    # checkpoint íŒŒì¼ë¡œë¶€í„° í•„ìš”í•œ layerë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤. 
    model.load_weights(os.path.join(model_dir, "bert_pretrain_32000.hdf5"), by_name=True)

    model.summary()
else:
    print('NO Pretrained Model')

"""## STEP 2. pretrained model finetune í•˜ê¸°"""

loss_fn = tf.keras.losses.sparse_categorical_crossentropy
acc_fn = tf.keras.metrics.sparse_categorical_accuracy

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4) # learning rate 5e-4(0.0005) ----> 3e-4(0.0003) ë‚®ì¶¤, ì•„ë‹´ ì˜µí‹°ë§ˆì´ì €ëŠ” ê·¸ëŒ€ë¡œ

best_acc = .0
patience = 0

# trainingê°’, validationì˜ ê°ê° accuracy, lossê°’ tracing

history = {'train_start_loss': [], 
           'train_end_loss': [], 
           'train_start_acc': [], 
           'train_end_acc': [],
           'val_start_loss': [], 
           'val_end_loss': [],
           'val_start_acc': [], 
           'val_end_acc': []}

for epoch in range(5): # ì´ 20ì—í­
    # train set í•™ìŠµ
    train_epoch(model, train_dataset, loss_fn, acc_fn, optimizer)
    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, train_dataset, loss_fn, acc_fn)
    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')
    
    # í•™ìŠµê³¼ì • ì‹œê°í™”
    history['train_start_loss'].append(start_loss)
    history['train_end_loss'].append(end_loss)
    history['train_start_acc'].append(start_acc)
    history['train_end_acc'].append(end_acc)
    
    # validation set í•™ìŠµ
    train_epoch(model, dev_dataset, loss_fn, acc_fn, optimizer)
    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, dev_dataset, loss_fn, acc_fn)
    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')
    
    # í•™ìŠµê³¼ì • ì‹œê°í™”
    history['val_start_loss'].append(start_loss)
    history['val_end_loss'].append(end_loss)
    history['val_start_acc'].append(start_acc)
    history['val_end_acc'].append(end_acc)

    acc = start_acc + end_acc
    if best_acc < acc:
        patience = 0
        best_acc = acc
        model.save_weights(os.path.join(data_dir, "korquad_bert_none_pretrain.hdf5"))
        print(f'save best model')
    else:
        patience += 1
    if 2 <= patience:
        print(f'early stopping')
        break

"""## STEP 3. Inference ìˆ˜í–‰í•˜ê¸°
finetune í•™ìŠµì´ ì™„ë£Œëœ modelì„ í™œìš©í•˜ì—¬ ì‹¤ì œ í€´ì¦ˆ í’€ì´ ê²°ê³¼ë¥¼ í™•ì¸í•´ ë³´ì

### ğŸ”¥
"""

def do_predict(model, question, context):
    """
    ì…ë ¥ì— ëŒ€í•œ ë‹µë³€ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    :param model: model
    :param question: ì…ë ¥ ë¬¸ìì—´
    :param context: ì…ë ¥ ë¬¸ìì—´
    """
    q_tokens = vocab.encode_as_pieces(question)[:args.max_query_length]
    c_tokens = vocab.encode_as_pieces(context)[:args.max_seq_length - len(q_tokens) - 3]
    tokens = ['[CLS]'] + q_tokens + ['[SEP]'] + c_tokens + ['[SEP]']
    token_ids = [vocab.piece_to_id(token) for token in tokens]
    segments = [0] * (len(q_tokens) + 2) + [1] * (len(c_tokens) + 1)

    y_start, y_end = model(np.array([token_ids]), np.array([segments]))
    # print(y_start, y_end)
    y_start_idx = K.argmax(y_start, axis=-1)[0].numpy()
    y_end_idx = K.argmax(y_end, axis=-1)[0].numpy()
    answer_tokens = tokens[y_start_idx:y_end_idx + 1]

    return vocab.decode_pieces(answer_tokens)

dev_json = os.path.join('/content/drive/MyDrive/data/TEST_preprocessing.json')

with open(dev_json) as f:
    for i, line in enumerate(f):
        data = json.loads(line)
        question = vocab.decode_pieces(data['question'])
        context = vocab.decode_pieces(data['context'])
        answer = data['answer']
        answer_predict = do_predict(model, question, context)
        if answer in answer_predict:
            print(i)
            print("ì§ˆë¬¸ : ", question)
            print("ì§€ë¬¸ : ", context)
            print("ì •ë‹µ : ", answer)
            print("ì˜ˆì¸¡ : ", answer_predict, "\n")
        if 100 < i:
            break

"""## STEP 4. í•™ìŠµ ê²½ê³¼ ì‹œê°í™” ë¹„êµ ë¶„ì„"""

# í›ˆë ¨ì´ ë§ˆë¬´ë¦¬ë˜ì—ˆìœ¼ë©´ ì‹œê°í™”ë¥¼ ì§„í–‰.

plt.figure(figsize=(16, 10))

plt.subplot(2, 2, 1)
plt.plot(history['train_start_loss'], 'g-', label='train_start_loss')
plt.plot(history['val_start_loss'], 'y--', label='val_start_loss')
plt.xlabel('Epoch')
plt.legend()
plt.title('start-loss')

plt.subplot(2, 2, 2)
plt.plot(history['train_end_loss'], 'g-', label='train_end_loss')
plt.plot(history['val_end_loss'], 'y--', label='val_end_loss')
plt.xlabel('Epoch')
plt.legend()
plt.title('end-loss')

plt.subplot(2, 2, 3)
plt.plot(history['train_start_acc'], 'g-', label='train_start_acc')
plt.plot(history['val_start_acc'], 'y--', label='val_start_acc')
plt.xlabel('Epoch')
plt.legend()
plt.title('start-accuracy')


plt.subplot(2, 2, 4)
plt.plot(history['train_end_acc'], 'g-', label='train_end_acc')
plt.plot(history['val_end_acc'], 'y--', label='val_end_acc')
plt.xlabel('Epoch')
plt.legend()
plt.title('end-accuracy')

plt.show()